{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Jon Dinu video 3.7\n",
    "import string\n",
    "import json \n",
    "import pickle as pkl\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pyspark as ps\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = ps.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_strings = ['the quick brown fox jumps over the brown fence.',\n",
    "              'the boy paints a tall fence brown!',\n",
    "              'basketball players are tall.',\n",
    "              'quick basketball players jump high']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the tokenize function - split the text into words\n",
    "import nltk, string\n",
    "\n",
    "def word_tokenizer(text):\n",
    "    tokens = [] # start with an empty list\n",
    "    \n",
    "    #parse all the words in the text and add to the list if it is not a stopword, punctuation or ''''''\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if word not in nltk.corpus.stopwords.words('english')\\\n",
    "        and word not in string.punctuation \\\n",
    "        and word != '``':  # this is specific to the document he was processing\n",
    "            tokens.append(word)\n",
    "            \n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['quick', 'brown', 'fox', 'jumps', 'brown', 'fence'],\n",
       " ['boy', 'paints', 'tall', 'fence', 'brown'],\n",
       " ['basketball', 'players', 'tall'],\n",
       " ['quick', 'basketball', 'players', 'jump', 'high']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_strings contains multiple documents/rows. We apply the tokeniser to each document/row. \n",
    "# and we create our RDD now so we can use map to perform our transforms for us. \n",
    "test_tokens = sc.parallelize(test_strings).map(word_tokenizer)\n",
    "test_tokens.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we want to vectorise the documents. We do this by counting the number of words but first we need the vocabulary\n",
    "vocab = test_tokens.flatMap(lambda words: words).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick',\n",
       " 'jump',\n",
       " 'high',\n",
       " 'brown',\n",
       " 'players',\n",
       " 'tall',\n",
       " 'paints',\n",
       " 'boy',\n",
       " 'jumps',\n",
       " 'basketball',\n",
       " 'fox',\n",
       " 'fence']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# So we need to write a function that can be applied to every row and will return a vector for every row \n",
    "# Some clever things below: \n",
    "# using the broadcast of the vocab list. \n",
    "# vector creation command! Nice\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "broadcastVocab = sc.broadcast(vocab.collect())\n",
    "def bow_vectorize(row_of_tokens):\n",
    "    \n",
    "    counts = Counter(row_of_tokens)\n",
    "    \n",
    "    vec = [counts[word] if word in counts else 0 for word in broadcastVocab.value]\n",
    "    return np.array(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector = test_tokens.map(bow_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 1]),\n",
       " array([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
       " array([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]),\n",
       " array([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "temp = test_tokens.map(lambda line: np.array(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['quick', 'brown', 'fox', 'jumps', 'brown', 'fence'], \n",
       "       dtype='|S5'), array(['boy', 'paints', 'tall', 'fence', 'brown'], \n",
       "       dtype='|S6'), array(['basketball', 'players', 'tall'], \n",
       "       dtype='|S10'), array(['quick', 'basketball', 'players', 'jump', 'high'], \n",
       "       dtype='|S10')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[21] at collect at <ipython-input-44-92b1853a940d>:1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
