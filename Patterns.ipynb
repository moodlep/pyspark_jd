{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "sc = ps.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From text file\n",
    "# rdd = sc.textFile('file://' + cwd + '/data/donors_choose/project_sample.csv')\n",
    "rdd_text = sc.textFile('opendata_projects000')\n",
    "\n",
    "# From a python list\n",
    "rdd_python = sc.parallelize([1,2,3,4,5,6])\n",
    "\n",
    "# From json\n",
    "\n",
    "# From the cloud\n",
    "# Replace <AWS_ACCESS_KEY_ID> and <AWS_SECRET_ACCESS_KEY> with your credentials\n",
    "link = 's3n://<AWS_ACCESS_KEY_ID>:<AWS_SECRET_ACCESS_KEY>@mortar-example-data/airline-data'\n",
    "rdd_cloud = sc.textFile(link)\n",
    "\n",
    "# CSV - use SparkCSV to read in csv data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a local Spark context for debug purposes\n",
    "sc = ps.SparkContext('local')\n",
    "\n",
    "# we can see that it uses all of the cores [*]\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distinct Counts - these are rdd functions. They give the count of a specific column. \n",
    "# Could we use a filter and sum to do this?\n",
    "rdd_dict.map(lambda row: row['_schoolid']).countApproxDistinct()\n",
    "rdd_dict.map(lambda row: row['_schoolid']).distinct().count()\n",
    "\n",
    "# More fancy counting below - see Accumulators and Counters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Dataframes in EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Null value checks and filling in nulls: \n",
    "df_force.filter(df_force['students_reached'].isNull()).select('students_reached', 'funding_status').collect()\n",
    "df_no_null = df_force.fillna(0, ['students_reached'])\n",
    "\n",
    "# Frequent Items\n",
    "freq_items = df_no_null.freqItems(['school_city', 'primary_focus_area', \\\n",
    "                                     'grade_level', 'poverty_level','resource_type'], 0.7).collect()\n",
    "\n",
    "df_no_null.freqItems(['num_donors'], .3).collect()[0]\n",
    "\n",
    "# Distributions and histograms\n",
    "\n",
    "df_no_null.groupby('funding_status').count().show()\n",
    "\n",
    "df_no_null.select('total_donations', 'num_donors', 'students_reached', \\\n",
    "                  df_no_null['total_price_excluding_optional_support'].alias('p_exclude'), \\\n",
    "                  df_no_null['total_price_including_optional_support'].alias('p_include')) \\\n",
    "          .describe().show()\n",
    "\n",
    "# Outliers\n",
    "# massive outliers, will skew histogram buckets\n",
    "outliers = price_rdd.top(3)\n",
    "\n",
    "# for continuous columns we can use Histogram RDD function\n",
    "hist = price_rdd.filter(lambda x: x not in outliers).histogram(100)\n",
    "\n",
    "# Plotting the histogram: \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%pylab inline\n",
    "def plot_rdd_hist(hist):\n",
    "    idx = []\n",
    "\n",
    "    for i in range(len(hist[0]) - 1):\n",
    "        idx.append((hist[0][i] + hist[0][i+1])/ 2)\n",
    "        \n",
    "    pd.DataFrame({'counts': hist[1], 'index': idx}).set_index('index').plot(figsize=(16,5))\n",
    "\n",
    "# Cheap histogram!\n",
    "cheap_histogram = price_rdd.filter(lambda x: x < 5000).histogram(100)\n",
    "\n",
    "'''The true power of Spark comes when we start passing data and operations between the local driver and the Spark context. \n",
    "In doing so we can combine operations that are most efficient in Spark on a cluster with local methods that operate \n",
    "on smaller data.\n",
    "Here we are using Spark to do the heavy lifting of creating distributiions of the relevant queries, and then \n",
    "explore/visualize the condensed data with pandas and matplotlib locally.'''\n",
    "# A generic Spark histogram plotter\n",
    "def spark_histogram(df, column):\n",
    "    donor_counts = df.groupby(column).count()\n",
    "    donor_df = donor_counts.toPandas()\n",
    "    donor_df[column] = donor_df.num_donors.astype(float)\n",
    "    return donor_df.sort(column).set_index(column).iloc[:50,:].plot(kind='bar', figsize=(14,5))\n",
    "\n",
    "spark_histogram(df_complete, 'num_donors')\n",
    "\n",
    "complete = df_complete.groupby('num_donors').count().toPandas()\n",
    "expired = df_expired.groupby('num_donors').count().toPandas()\n",
    "\n",
    "# correlations - this is on a dataframe so be aware that df is a dataframe....\n",
    "df_no_null.stat.corr('total_price_excluding_optional_support', 'num_donors')\n",
    "\n",
    "# Crosstabs.... what are these? \n",
    "\n",
    "# categorical/boolean fields can give valuable facets (crosstabs)\n",
    "df_no_null.crosstab('resource_type', 'funding_status').show()\n",
    "df_no_null.crosstab('primary_focus_area', 'resource_type').show()\n",
    "\n",
    "# Output below: like a pivot table in Excel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+----------------------------+-----+---------+-----------+-------+\n",
    "|resource_type_funding_status| live|completed|reallocated|expired|\n",
    "+----------------------------+-----+---------+-----------+-------+\n",
    "|                        null|    2|       28|          0|     18|\n",
    "|                       Other| 4542|    54610|        747|  22550|\n",
    "|                       Books| 5982|   118810|       1527|  34554|\n",
    "|                    Visitors|  102|      806|          6|    341|\n",
    "|                    Supplies|11939|   185870|       2602|  63406|\n",
    "|                       Trips|  347|     4381|         62|   1474|\n",
    "|                  Technology|18957|   150500|       2256|  85510|\n",
    "+----------------------------+-----+---------+-----------+-------+\n",
    "\n",
    "+--------------------------------+-----+--------+-----+----------+------+--------+----+\n",
    "|primary_focus_area_resource_type|Trips|Visitors|Other|Technology| Books|Supplies|null|\n",
    "+--------------------------------+-----+--------+-----+----------+------+--------+----+\n",
    "|             Literacy & Language|  630|     228|32795|    109605|127282|   75924|   4|\n",
    "|                            null|    0|       0|    0|         0|     1|       0|  41|\n",
    "|                Applied Learning| 1197|     104| 9429|     17869|  4863|   22596|   0|\n",
    "|                  Math & Science| 1902|     323|16353|     75189| 11746|   89101|   3|\n",
    "|                Music & The Arts|  947|     441| 8305|     19289|  2883|   37804|   0|\n",
    "|                 Health & Sports|  159|      54| 4633|      3054|   432|   12970|   0|\n",
    "|                   Special Needs|  241|      32| 7636|     19359|  4112|   17151|   0|\n",
    "|                History & Civics| 1188|      73| 3298|     12858|  9554|    8271|   0|\n",
    "+--------------------------------+-----+--------+-----+----------+------+--------+----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rdd value_counts()\n",
    "rdd_dict.map(lambda d: (d['teacher_ny_teaching_fellow'], 1)).reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing or null values\n",
    "\n",
    "To get more info on this, go to https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/sql/DataFrameNaFunctions.html\n",
    "\n",
    "This site lists all the functions that are available like dropna(column name), fillna(), replace(). These are attached to the dataframe structure. \n",
    "There is also column.isNull(). An example is illustrated below - research this when required. \n",
    "\n",
    "**Accumulators** are used to find how many null values there are in columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['45', '3', '27.1', 'Jonathan', 'Dinu', '\"Galvanize\"', '\"San Francisco ', ' CA\"', ' 26']\n",
      "Naive splitting creates 9 fields\n",
      "\n",
      "['45', '3', '27.1', 'Jonathan', 'Dinu', 'Galvanize', 'San Francisco , CA', ' 26']\n",
      "Using the built in CSV library creates 8 fields\n"
     ]
    }
   ],
   "source": [
    "# Using csv libraries to split csv format lines so commas in strings are not a problem\n",
    "\n",
    "import csv\n",
    "record = '45,3,27.1,Jonathan,Dinu,\"Galvanize\",\"San Francisco , CA\", 26'\n",
    "naive = record.split(',')\n",
    "csv_lib = csv.reader(['45,3,27.1,Jonathan,Dinu,\"Galvanize\",\"San Francisco , CA\", 26']).next()\n",
    "print naive\n",
    "print \"Naive splitting creates {0} fields\".format(len(naive)) + \"\\n\"\n",
    "print csv_lib\n",
    "print \"Using the built in CSV library creates {0} fields\".format(len(csv_lib))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type conversion utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Format the date string \n",
    "# http://strftime.org/\n",
    "from datetime import datetime\n",
    "def date_parse(datestring):\n",
    "    return None if datestring == '' else str(datetime.strptime(datestring, '%Y-%m-%d'))\n",
    "\n",
    "# Return booleans the way you need to: \n",
    "def boolean_map(field):\n",
    "    if field == 't':\n",
    "        return True\n",
    "    elif field == 'f':\n",
    "        return False\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trick: Count of records that match this filter\n",
    "# sum the boolean returned (1 or 0) to get a count of records in this filter condition. \n",
    "\n",
    "rows_in_error = rdd_csv.filter(lambda row: len(row) != header_columns).sum()\n",
    "\n",
    "# Trick: foreach \n",
    "# calling foreach on an rdd allows you to run any function over all records without any return values. \n",
    "# This is an exception that tests if the row is malformed. \n",
    "rdd_csv.foreach(throw_exception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark more admin-type features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing the debug string for an rdd to view the lineage\n",
    "print destination_rdd.toDebugString()\n",
    "\n",
    "# Caching rdds that will be use a lot:\n",
    "destination_rdd.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful python commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# str() converts the input to a string \n",
    "rdd_no_dups = rdd_csv_corrrect.map(lambda row: str(row)).distinct()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
