{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes in Spark\n",
    "\n",
    "A dataframe is a different type of datastructure that Spark contains, vs the RDD. \n",
    "Is it distributed too? \n",
    "From KDNuggets: (http://www.kdnuggets.com/2016/02/apache-spark-rdd-dataframe-dataset.html)\n",
    "*Spark 1.0 used the RDD API but in the past twelve months, two new alternative and incompatible APIs have been introduced. Spark 1.3 introduced the radically different DataFrame API and the recently released Spark 1.6 release introduces a preview of the new Dataset API. \n",
    "Many existing Spark developers will be wondering whether to jump from RDDs directly to the Dataset API, or whether to first move to the DataFrame API. Newcomers to Spark will have to choose which API to start learning with. \n",
    "*\n",
    "\n",
    "Dataframes vs RDD - DF allows you to work with the whole row, and with whole columns. \n",
    "More like pandas. See KDNuggets article. \n",
    "\n",
    "### R quirks: \n",
    "\n",
    "<- is the same as =\n",
    "R's key datastructure is the dataframe which loads tabular csv like data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a dataframe called flights: \n",
    "\n",
    "flights = spark.read.csv('492610425_T_ONTIME.csv',header='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(YEAR,StringType,true),StructField(UNIQUE_CARRIER,StringType,true),StructField(ORIGIN_AIRPORT_ID,StringType,true),StructField(DEST_AIRPORT_ID,StringType,true),StructField(DEP_DELAY,StringType,true),StructField(ARR_DELAY,StringType,true),StructField(CANCELLED,StringType,true),StructField(_c7,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "print flights.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting and casting columns\n",
    "Create a subset of your data as soon as you can to reduce the size. \n",
    "You want to work with the most concise form as soon as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove that last column - actually just select the columns you want - uses an sql like syntax in the select!\n",
    "# use col() to select the column and apply a cast to float. \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "subset = flights.select(col('DEP_DELAY').cast(\"float\"), col('ARR_DELAY').cast(\"float\"), 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.cache of DataFrame[DEP_DELAY: float, ARR_DELAY: float, ORIGIN_AIRPORT_ID: string, DEST_AIRPORT_ID: string]>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cache the result for later\n",
    "subset.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEP_DELAY=-5.0, ARR_DELAY=7.0, ORIGIN_AIRPORT_ID=u'12478', DEST_AIRPORT_ID=u'12892'),\n",
       " Row(DEP_DELAY=-10.0, ARR_DELAY=-19.0, ORIGIN_AIRPORT_ID=u'12478', DEST_AIRPORT_ID=u'12892'),\n",
       " Row(DEP_DELAY=-7.0, ARR_DELAY=-39.0, ORIGIN_AIRPORT_ID=u'12478', DEST_AIRPORT_ID=u'12892')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now our data is all string in string format. Use dtype to determine the type here. \n",
    "We need to convert the times to floats. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DEP_DELAY', 'float'),\n",
       " ('ARR_DELAY', 'float'),\n",
       " ('ORIGIN_AIRPORT_ID', 'string'),\n",
       " ('DEST_AIRPORT_ID', 'string')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Groupby by the destimation airport\n",
    "groupedDestMean = subset.groupBy(\"DEST_AIRPORT_ID\").mean(\"ARR_DELAY\")\n",
    "groupedOriginMean = subset.groupBy(\"ORIGIN_AIRPORT_ID\").mean(\"DEP_DELAY\")\n",
    "\n",
    "# TBD - don't know how to display this result! mean() does not return an RDD... documentation needs to be accessed. \n",
    "#https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.GroupedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
